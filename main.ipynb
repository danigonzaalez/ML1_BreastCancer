{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30fc954b",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "In this section, we import the dataset and prepare it for our models. The preprocessing pipeline includes splitting the data into **Training**, **Validation**, and **Test** sets.\n",
    "\n",
    "Crucially, feature normalization will be applied by fitting the scaler **only on the training set**. These statistics are then applied to transform the validation and test sets. This approach is strictly necessary to avoid **data leakage**, ensuring that our model remains unbiased and effectively evaluates unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6570d925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using Pkg\n",
    "# Ensure required packages (uncomment to install if needed)\n",
    "# Pkg.add([\n",
    "#     \"MLJ\", \n",
    "#     \"MLJBase\", \n",
    "#     \"MLJModels\", \n",
    "#     \"MLJEnsembles\", \n",
    "#     \"MLJLinearModels\", \n",
    "#     \"DecisionTree\", \n",
    "#     \"MLJDecisionTreeInterface\", \n",
    "#     \"NaiveBayes\", \n",
    "#     \"EvoTrees\", \n",
    "#     \"CategoricalArrays\", \n",
    "#     \"Random\",\n",
    "#     \"LIBSVM\",           \n",
    "#     \"Plots\",            \n",
    "#     \"MLJModelInterface\", \n",
    "#     \"CSV\",              \n",
    "#     \"DataFrames\",      \n",
    "#     \"MLJFlux\", \n",
    "#     \"UrlDownload\",      \n",
    "#     \"XGBoost\"    \n",
    "# ])\n",
    "\n",
    "include(\"Utils.jl\")\n",
    "using .Utils\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using Random\n",
    "using MLJ\n",
    "\n",
    "Random.seed!(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87376935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Updated DataFrame Headers ---\n",
      "--- Dataset Summary ---\n",
      "Total Rows: 569\n",
      "Total Columns: 32\n",
      "Target variable (y) shape: (569,)\n",
      "Features matrix (X) shape: (569, 30)\n",
      "\n",
      "--- Variable Information (First 5 Features) ---\n",
      "\n",
      "--- Target Distribution ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>30×7 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">5 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variable</th><th style = \"text-align: left;\">mean</th><th style = \"text-align: left;\">min</th><th style = \"text-align: left;\">median</th><th style = \"text-align: left;\">max</th><th style = \"text-align: left;\">nmissing</th><th style = \"text-align: left;\">eltype</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Symbol\" style = \"text-align: left;\">Symbol</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"DataType\" style = \"text-align: left;\">DataType</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">radius_mean</td><td style = \"text-align: right;\">14.1273</td><td style = \"text-align: right;\">6.981</td><td style = \"text-align: right;\">13.37</td><td style = \"text-align: right;\">28.11</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">texture_mean</td><td style = \"text-align: right;\">19.2896</td><td style = \"text-align: right;\">9.71</td><td style = \"text-align: right;\">18.84</td><td style = \"text-align: right;\">39.28</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">perimeter_mean</td><td style = \"text-align: right;\">91.969</td><td style = \"text-align: right;\">43.79</td><td style = \"text-align: right;\">86.24</td><td style = \"text-align: right;\">188.5</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">area_mean</td><td style = \"text-align: right;\">654.889</td><td style = \"text-align: right;\">143.5</td><td style = \"text-align: right;\">551.1</td><td style = \"text-align: right;\">2501.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">smoothness_mean</td><td style = \"text-align: right;\">0.0963603</td><td style = \"text-align: right;\">0.05263</td><td style = \"text-align: right;\">0.09587</td><td style = \"text-align: right;\">0.1634</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">compactness_mean</td><td style = \"text-align: right;\">0.104341</td><td style = \"text-align: right;\">0.01938</td><td style = \"text-align: right;\">0.09263</td><td style = \"text-align: right;\">0.3454</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">concavity_mean</td><td style = \"text-align: right;\">0.0887993</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.06154</td><td style = \"text-align: right;\">0.4268</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">concave_points_mean</td><td style = \"text-align: right;\">0.0489191</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.0335</td><td style = \"text-align: right;\">0.2012</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">symmetry_mean</td><td style = \"text-align: right;\">0.181162</td><td style = \"text-align: right;\">0.106</td><td style = \"text-align: right;\">0.1792</td><td style = \"text-align: right;\">0.304</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">fractal_dimension_mean</td><td style = \"text-align: right;\">0.0627976</td><td style = \"text-align: right;\">0.04996</td><td style = \"text-align: right;\">0.06154</td><td style = \"text-align: right;\">0.09744</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">radius_se</td><td style = \"text-align: right;\">0.405172</td><td style = \"text-align: right;\">0.1115</td><td style = \"text-align: right;\">0.3242</td><td style = \"text-align: right;\">2.873</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">texture_se</td><td style = \"text-align: right;\">1.21685</td><td style = \"text-align: right;\">0.3602</td><td style = \"text-align: right;\">1.108</td><td style = \"text-align: right;\">4.885</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">perimeter_se</td><td style = \"text-align: right;\">2.86606</td><td style = \"text-align: right;\">0.757</td><td style = \"text-align: right;\">2.287</td><td style = \"text-align: right;\">21.98</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">19</td><td style = \"text-align: left;\">symmetry_se</td><td style = \"text-align: right;\">0.0205423</td><td style = \"text-align: right;\">0.007882</td><td style = \"text-align: right;\">0.01873</td><td style = \"text-align: right;\">0.07895</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">20</td><td style = \"text-align: left;\">fractal_dimension_se</td><td style = \"text-align: right;\">0.0037949</td><td style = \"text-align: right;\">0.0008948</td><td style = \"text-align: right;\">0.003187</td><td style = \"text-align: right;\">0.02984</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">21</td><td style = \"text-align: left;\">radius_worst</td><td style = \"text-align: right;\">16.2692</td><td style = \"text-align: right;\">7.93</td><td style = \"text-align: right;\">14.97</td><td style = \"text-align: right;\">36.04</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">22</td><td style = \"text-align: left;\">texture_worst</td><td style = \"text-align: right;\">25.6772</td><td style = \"text-align: right;\">12.02</td><td style = \"text-align: right;\">25.41</td><td style = \"text-align: right;\">49.54</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">23</td><td style = \"text-align: left;\">perimeter_worst</td><td style = \"text-align: right;\">107.261</td><td style = \"text-align: right;\">50.41</td><td style = \"text-align: right;\">97.66</td><td style = \"text-align: right;\">251.2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">24</td><td style = \"text-align: left;\">area_worst</td><td style = \"text-align: right;\">880.583</td><td style = \"text-align: right;\">185.2</td><td style = \"text-align: right;\">686.5</td><td style = \"text-align: right;\">4254.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">25</td><td style = \"text-align: left;\">smoothness_worst</td><td style = \"text-align: right;\">0.132369</td><td style = \"text-align: right;\">0.07117</td><td style = \"text-align: right;\">0.1313</td><td style = \"text-align: right;\">0.2226</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">26</td><td style = \"text-align: left;\">compactness_worst</td><td style = \"text-align: right;\">0.254265</td><td style = \"text-align: right;\">0.02729</td><td style = \"text-align: right;\">0.2119</td><td style = \"text-align: right;\">1.058</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">27</td><td style = \"text-align: left;\">concavity_worst</td><td style = \"text-align: right;\">0.272188</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.2267</td><td style = \"text-align: right;\">1.252</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">28</td><td style = \"text-align: left;\">concave_points_worst</td><td style = \"text-align: right;\">0.114606</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0.09993</td><td style = \"text-align: right;\">0.291</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">29</td><td style = \"text-align: left;\">symmetry_worst</td><td style = \"text-align: right;\">0.290076</td><td style = \"text-align: right;\">0.1565</td><td style = \"text-align: right;\">0.2822</td><td style = \"text-align: right;\">0.6638</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">30</td><td style = \"text-align: left;\">fractal_dimension_worst</td><td style = \"text-align: right;\">0.0839458</td><td style = \"text-align: right;\">0.05504</td><td style = \"text-align: right;\">0.08004</td><td style = \"text-align: right;\">0.2075</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64 & Float64 & Float64 & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & radius\\_mean & 14.1273 & 6.981 & 13.37 & 28.11 & 0 & Float64 \\\\\n",
       "\t2 & texture\\_mean & 19.2896 & 9.71 & 18.84 & 39.28 & 0 & Float64 \\\\\n",
       "\t3 & perimeter\\_mean & 91.969 & 43.79 & 86.24 & 188.5 & 0 & Float64 \\\\\n",
       "\t4 & area\\_mean & 654.889 & 143.5 & 551.1 & 2501.0 & 0 & Float64 \\\\\n",
       "\t5 & smoothness\\_mean & 0.0963603 & 0.05263 & 0.09587 & 0.1634 & 0 & Float64 \\\\\n",
       "\t6 & compactness\\_mean & 0.104341 & 0.01938 & 0.09263 & 0.3454 & 0 & Float64 \\\\\n",
       "\t7 & concavity\\_mean & 0.0887993 & 0.0 & 0.06154 & 0.4268 & 0 & Float64 \\\\\n",
       "\t8 & concave\\_points\\_mean & 0.0489191 & 0.0 & 0.0335 & 0.2012 & 0 & Float64 \\\\\n",
       "\t9 & symmetry\\_mean & 0.181162 & 0.106 & 0.1792 & 0.304 & 0 & Float64 \\\\\n",
       "\t10 & fractal\\_dimension\\_mean & 0.0627976 & 0.04996 & 0.06154 & 0.09744 & 0 & Float64 \\\\\n",
       "\t11 & radius\\_se & 0.405172 & 0.1115 & 0.3242 & 2.873 & 0 & Float64 \\\\\n",
       "\t12 & texture\\_se & 1.21685 & 0.3602 & 1.108 & 4.885 & 0 & Float64 \\\\\n",
       "\t13 & perimeter\\_se & 2.86606 & 0.757 & 2.287 & 21.98 & 0 & Float64 \\\\\n",
       "\t14 & area\\_se & 40.3371 & 6.802 & 24.53 & 542.2 & 0 & Float64 \\\\\n",
       "\t15 & smoothness\\_se & 0.00704098 & 0.001713 & 0.00638 & 0.03113 & 0 & Float64 \\\\\n",
       "\t16 & compactness\\_se & 0.0254781 & 0.002252 & 0.02045 & 0.1354 & 0 & Float64 \\\\\n",
       "\t17 & concavity\\_se & 0.0318937 & 0.0 & 0.02589 & 0.396 & 0 & Float64 \\\\\n",
       "\t18 & concave\\_points\\_se & 0.0117961 & 0.0 & 0.01093 & 0.05279 & 0 & Float64 \\\\\n",
       "\t19 & symmetry\\_se & 0.0205423 & 0.007882 & 0.01873 & 0.07895 & 0 & Float64 \\\\\n",
       "\t20 & fractal\\_dimension\\_se & 0.0037949 & 0.0008948 & 0.003187 & 0.02984 & 0 & Float64 \\\\\n",
       "\t21 & radius\\_worst & 16.2692 & 7.93 & 14.97 & 36.04 & 0 & Float64 \\\\\n",
       "\t22 & texture\\_worst & 25.6772 & 12.02 & 25.41 & 49.54 & 0 & Float64 \\\\\n",
       "\t23 & perimeter\\_worst & 107.261 & 50.41 & 97.66 & 251.2 & 0 & Float64 \\\\\n",
       "\t24 & area\\_worst & 880.583 & 185.2 & 686.5 & 4254.0 & 0 & Float64 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m30×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable                \u001b[0m\u001b[1m mean        \u001b[0m\u001b[1m min         \u001b[0m\u001b[1m median     \u001b[0m\u001b[1m max     \u001b[0m ⋯\n",
       "     │\u001b[90m Symbol                  \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64 \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ radius_mean               14.1273       6.981       13.37        28.11  ⋯\n",
       "   2 │ texture_mean              19.2896       9.71        18.84        39.28\n",
       "   3 │ perimeter_mean            91.969       43.79        86.24       188.5\n",
       "   4 │ area_mean                654.889      143.5        551.1       2501.0\n",
       "   5 │ smoothness_mean            0.0963603    0.05263      0.09587      0.163 ⋯\n",
       "   6 │ compactness_mean           0.104341     0.01938      0.09263      0.345\n",
       "   7 │ concavity_mean             0.0887993    0.0          0.06154      0.426\n",
       "   8 │ concave_points_mean        0.0489191    0.0          0.0335       0.201\n",
       "  ⋮  │            ⋮                  ⋮            ⋮           ⋮           ⋮    ⋱\n",
       "  24 │ area_worst               880.583      185.2        686.5       4254.0   ⋯\n",
       "  25 │ smoothness_worst           0.132369     0.07117      0.1313       0.222\n",
       "  26 │ compactness_worst          0.254265     0.02729      0.2119       1.058\n",
       "  27 │ concavity_worst            0.272188     0.0          0.2267       1.252\n",
       "  28 │ concave_points_worst       0.114606     0.0          0.09993      0.291 ⋯\n",
       "  29 │ symmetry_worst             0.290076     0.1565       0.2822       0.663\n",
       "  30 │ fractal_dimension_worst    0.0839458    0.05504      0.08004      0.207\n",
       "\u001b[36m                                                   3 columns and 15 rows omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>2×2 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Diagnosis</th><th style = \"text-align: left;\">nrow</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String1\" style = \"text-align: left;\">String1</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">M</td><td style = \"text-align: right;\">212</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">B</td><td style = \"text-align: right;\">357</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& Diagnosis & nrow\\\\\n",
       "\t\\hline\n",
       "\t& String1 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & M & 212 \\\\\n",
       "\t2 & B & 357 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m2×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Diagnosis \u001b[0m\u001b[1m nrow  \u001b[0m\n",
       "     │\u001b[90m String1   \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "─────┼──────────────────\n",
       "   1 │ M            212\n",
       "   2 │ B            357"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load data from wdbc.data file\n",
    "df = CSV.read(\"wdbc.data\", DataFrame, header=false)\n",
    "\n",
    "new_names = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    # The Mean (first 10 features)\n",
    "    \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n",
    "    \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
    "    # The Standard Error (next 10 features)\n",
    "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\",\n",
    "    \"compactness_se\", \"concavity_se\", \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
    "    # The \"Worst\" or Largest (last 10 features)\n",
    "    \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",\n",
    "    \"compactness_worst\", \"concavity_worst\", \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"\n",
    "]\n",
    "rename!(df, new_names)\n",
    "\n",
    "# 4. Verify the changes\n",
    "# Let's look at the first 3 rows and the new header\n",
    "println(\"--- Updated DataFrame Headers ---\")\n",
    "first(df, 3)\n",
    "\n",
    "# 2. Data separation (mimicking the Python script)\n",
    "y = df[:, 2]\n",
    "x = df[:, 3:end]\n",
    "\n",
    "# 3. Metadata equivalent\n",
    "println(\"--- Dataset Summary ---\")\n",
    "println(\"Total Rows: \", nrow(df))\n",
    "println(\"Total Columns: \", ncol(df))\n",
    "println(\"Target variable (y) shape: \", size(y))\n",
    "println(\"Features matrix (X) shape: \", size(x))\n",
    "\n",
    "println(\"\\n--- Variable Information (First 5 Features) ---\")\n",
    "display(describe(x))\n",
    "println(\"\\n--- Target Distribution ---\")\n",
    "display(combine(groupby(DataFrame(Diagnosis=y), :Diagnosis), nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2596b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Summary (Raw Data) ---\n",
      "Features (X) dimension: (569, 30)\n",
      "Targets (y_encoded) dimension: (569, 1)\n",
      "Mean of first feature column (Raw): 14.127291739894556\n",
      "Max of first feature column (Raw): 28.11\n",
      "Targets (first 5 encoded): \n",
      "Bool[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# This cell converts the input features 'x' into a Matrix format and identifies the unique classes\n",
    "# from the target vector 'y'. It then performs One-Hot Encoding on the targets. Finally, it prints\n",
    "# a summary of the raw data dimensions and basic statistics (mean, max) of the first feature\n",
    "# column to verify the state of the data before any normalization is applied.\n",
    "\n",
    "# 1. Convert to Matrix and Encode Targets (NOT NORMALIZED YET)\n",
    "x = Matrix(x)\n",
    "\n",
    "classes = unique(y)\n",
    "y_encoded = Utils.oneHotEncoding(y, classes)\n",
    "\n",
    "println(\"--- Data Summary (Raw Data) ---\")\n",
    "println(\"Features (X) dimension: \", size(x))\n",
    "println(\"Targets (y_encoded) dimension: \", size(y_encoded))\n",
    "# You will see real values (e.g., 14.12), not values between 0 and 1 yet\n",
    "println(\"Mean of first feature column (Raw): \", mean(x[:, 1])) \n",
    "println(\"Max of first feature column (Raw): \", maximum(x[:, 1]))\n",
    "println(\"Targets (first 5 encoded): \\n\", first(y_encoded, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17788364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split Summary (RAW DATA) ---\n",
      "Total instances: 569\n",
      "Training set size: 456\n",
      "Test set size: 113\n",
      "\n",
      "--- Raw Data Verification ---\n",
      "Max of first feature (Train): 28.11\n",
      "Mean of first feature (Train): 14.08660087719298\n"
     ]
    }
   ],
   "source": [
    "# This cell performs data splitting. It divides the data into training and test sets.\n",
    "# Unlike the previous version, we do NOT create a global validation set here.\n",
    "# IMPORTANT: In this step, we DO NOT normalize yet. We keep x_train and x_test \n",
    "# with their raw values. This allows the Cross-Validation process (in Utils) to \n",
    "# perform its own internal normalization per fold, ensuring strict rigorousness.\n",
    "\n",
    "# 2. Splitting (Raw Data) - 80% Train, 20% Test\n",
    "num_instances = size(x, 1)\n",
    "test_ratio = 0.2 \n",
    "\n",
    "# A. Get the indices (Using holdOut with 2 arguments returns Train and Test indices)\n",
    "train_idx, test_idx = Utils.holdOut(num_instances, test_ratio)\n",
    "\n",
    "# B. Create the subsets (RAW DATA - NOT NORMALIZED)\n",
    "x_train = x[train_idx, :]\n",
    "y_train = y_encoded[train_idx, :]\n",
    "\n",
    "x_test = x[test_idx, :]\n",
    "y_test = y_encoded[test_idx, :]\n",
    "\n",
    "println(\"--- Split Summary (RAW DATA) ---\")\n",
    "println(\"Total instances: $num_instances\")\n",
    "println(\"Training set size: $(size(x_train, 1))\")\n",
    "println(\"Test set size: $(size(x_test, 1))\")\n",
    "\n",
    "# Verification: The values should be real numbers (e.g., > 1.0), not normalized yet\n",
    "println(\"\\n--- Raw Data Verification ---\")\n",
    "println(\"Max of first feature (Train): \", maximum(x_train[:, 1]))\n",
    "println(\"Mean of first feature (Train): \", mean(x_train[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0848e4",
   "metadata": {},
   "source": [
    "### Approach 1: All Dataset with CV\n",
    "\n",
    "In this first approach, we will utilize **all available features** present in the dataset. \n",
    "\n",
    "This method serves as a baseline, allowing us to evaluate model performance using the complete set of variables without applying any feature selection or dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bcac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INITIALIZING APPROACH: ALL DATA ---\n",
      "All ready: Targets prepared and CV (10 folds) generated.\n"
     ]
    }
   ],
   "source": [
    "# This cell sets up the configuration for \"Approach 1\" (using all data). It prepares the \n",
    "# target variables by converting them into vector format as required for model training. \n",
    "# It also initializes a dictionary to store the best hyperparameters found later. Finally, \n",
    "# it sets a fixed random seed and generates indices for 10-fold cross-validation to ensure \n",
    "# that the splits are reproducible.\n",
    "\n",
    "# --- APPROACH 1 CONFIGURATION ---\n",
    "println(\"--- INITIALIZING APPROACH: ALL DATA ---\")\n",
    "\n",
    "# 1. Prepare vector targets (required for MLJ)\n",
    "# Assuming the positive class is 'true' in column 1\n",
    "targets_train_vec = vec(y_train[:, 1]) \n",
    "targets_test_vec  = vec(y_test[:, 1])\n",
    "\n",
    "# 2. Dictionary to save the best hyperparameters for each type\n",
    "# (This is what we will use later for the Ensemble)\n",
    "best_models = Dict()\n",
    "\n",
    "# 3. Generate cross-validation indices (10 folds)\n",
    "# Use a fixed seed so the folds are always the same\n",
    "k_folds = 10\n",
    "cv_indices = Utils.crossvalidation(targets_train_vec, k_folds)\n",
    "\n",
    "println(\"All ready: Targets prepared and CV (10 folds) generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a65441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. ANN (Fast Mode) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(30 => 5, σ)\n",
      "│   summary(x) = 30×328 adjoint(::Matrix{Float64}) with eltype Float64\n",
      "└ @ Flux C:\\Users\\Hugo\\.julia\\packages\\Flux\\uRn8o\\src\\layers\\stateless.jl:60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topology: [5] -> Acc: 0.8953\n",
      "Topology: [10] -> Acc: 0.9648\n",
      "Topology: [20] -> Acc: 0.9671\n",
      "Topology: [5, 5] -> Acc: 0.8886\n",
      "Topology: [10, 10] -> Acc: 0.9737\n",
      "Topology: [20, 10] -> Acc: 0.9782\n",
      "Topology: [10, 5] -> Acc: 0.9782\n",
      "Topology: [30, 15] -> Acc: 0.9739\n",
      ">> BEST ANN: [20, 10] | Acc: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 5 entries:\n",
       "  \"maxEpochs\"       => 200\n",
       "  \"learningRate\"    => 0.01\n",
       "  \"topology\"        => [20, 10]\n",
       "  \"validationRatio\" => 0.2\n",
       "  \"numExecutions\"   => 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell iterates through a list of different Neural Network topologies to find the best one.\n",
    "# It uses a \"Fast Mode\" configuration with fewer epochs and a single execution per fold to\n",
    "# significantly speed up the hyperparameter tuning process. The best performing topology\n",
    "# (based on accuracy) is identified and saved to the best_models dictionary.\n",
    "\n",
    "println(\"\\n--- 1. ANN (Fast Mode) ---\")\n",
    "topologies = [[5], [10], [20], [5,5], [10,10], [20,10], [10,5], [30,15]]\n",
    "best_ann_acc = 0.0\n",
    "best_ann_params = nothing\n",
    "\n",
    "for topo in topologies\n",
    "    # Parameters optimized for speed\n",
    "    hyperparameters = Dict(\n",
    "        \"topology\" => topo,\n",
    "        \"maxEpochs\" => 200,       # Few epochs to test quickly\n",
    "        \"learningRate\" => 0.01,\n",
    "        \"validationRatio\" => 0.2,\n",
    "        \"numExecutions\" => 1      # <--- THIS MAKES IT TAKE SECONDS INSTEAD OF HOURS\n",
    "    )\n",
    "    \n",
    "    metrics, _ = Utils.modelCompilation(:ANN, hyperparameters, (x_train, targets_train_vec), cv_indices)\n",
    "    acc = metrics[1]\n",
    "    \n",
    "    # Print accuracy for the current topology\n",
    "    println(\"Topology: $topo -> Acc: $(round(acc, digits=4))\")\n",
    "    \n",
    "    if acc > best_ann_acc\n",
    "        global best_ann_acc = acc\n",
    "        global best_ann_params = hyperparameters\n",
    "    end\n",
    "end\n",
    "println(\">> BEST ANN: $(best_ann_params[\"topology\"]) | Acc: $(round(best_ann_acc, digits=4))\")\n",
    "best_models[:ANN] = best_ann_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26589e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Training SVM ---\n",
      "Kernel: rbf, C: 1.0 -> Acc: 0.9647\n",
      "Kernel: rbf, C: 10.0 -> Acc: 0.9759\n",
      "Kernel: rbf, C: 100.0 -> Acc: 0.9671\n",
      "Kernel: linear, C: 1.0 -> Acc: 0.9758\n",
      "Kernel: linear, C: 10.0 -> Acc: 0.9694\n",
      "Kernel: poly, C: 1.0 -> Acc: 0.956\n",
      "Kernel: poly, C: 10.0 -> Acc: 0.9691\n",
      "Kernel: sigmoid, C: 1.0 -> Acc: 0.9626\n",
      ">> SVM WINNER: Kernel rbf, C=10.0 with Acc: 0.9758893280632412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 2 entries:\n",
       "  \"C\"      => 10.0\n",
       "  \"kernel\" => \"rbf\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell trains Support Vector Machine (SVM) models using 8 different configurations. \n",
    "# It iterates through various combinations of kernel types (RBF, Linear, Poly, Sigmoid) \n",
    "# and regularization parameters 'C'. For each configuration, it evaluates performance using \n",
    "# cross-validation, identifies the best configuration based on accuracy, and stores the \n",
    "# winning parameters in the best_models dictionary.\n",
    "\n",
    "println(\"\\n--- 2. Training SVM ---\")\n",
    "\n",
    "# 8 Configurations\n",
    "configs_svm = [\n",
    "    (\"rbf\", 1.0), (\"rbf\", 10.0), (\"rbf\", 100.0),\n",
    "    (\"linear\", 1.0), (\"linear\", 10.0),\n",
    "    (\"poly\", 1.0), (\"poly\", 10.0),\n",
    "    (\"sigmoid\", 1.0)\n",
    "]\n",
    "\n",
    "best_svm_acc = 0.0\n",
    "best_svm_params = nothing\n",
    "\n",
    "for (k, c) in configs_svm\n",
    "    params = Dict(\"kernel\" => k, \"C\" => c)\n",
    "    metrics, _ = Utils.modelCompilation(:SVC, params, (x_train, targets_train_vec), cv_indices)\n",
    "    acc = metrics[1]\n",
    "    \n",
    "    # Print accuracy for the current configuration\n",
    "    println(\"Kernel: $k, C: $c -> Acc: $(round(acc, digits=4))\")\n",
    "    \n",
    "    if acc > best_svm_acc\n",
    "        global best_svm_acc = acc\n",
    "        global best_svm_params = params\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\">> SVM WINNER: Kernel $(best_svm_params[\"kernel\"]), C=$(best_svm_params[\"C\"]) with Acc: $best_svm_acc\")\n",
    "best_models[:SVM] = best_svm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196034b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Training Decision Trees ---\n",
      "Depth: 3 -> Acc: 0.9539\n",
      "Depth: 4 -> Acc: 0.9473\n",
      "Depth: 5 -> Acc: 0.9518\n",
      "Depth: 7 -> Acc: 0.9518\n",
      "Depth: 9 -> Acc: 0.9518\n",
      "Depth: 12 -> Acc: 0.9518\n",
      ">> DT WINNER: Depth 3 with Acc: 0.9538581466842336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Int64} with 1 entry:\n",
       "  \"max_depth\" => 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell trains Decision Tree classifiers to find the optimal tree depth.\n",
    "# It iterates through a predefined list of maximum depths (from 3 to 12) and evaluates\n",
    "# each using the model compilation utility with cross-validation. It prints the accuracy\n",
    "# for each depth and stores the best-performing configuration in the best_models dictionary.\n",
    "\n",
    "println(\"\\n--- 3. Training Decision Trees ---\")\n",
    "\n",
    "# 6 Depths\n",
    "depths = [3, 4, 5, 7, 9, 12]\n",
    "\n",
    "best_dt_acc = 0.0\n",
    "best_dt_params = nothing\n",
    "\n",
    "for d in depths\n",
    "    params = Dict(\"max_depth\" => d)\n",
    "    metrics, _ = Utils.modelCompilation(:DecisionTreeClassifier, params, (x_train, targets_train_vec), cv_indices)\n",
    "    acc = metrics[1]\n",
    "    \n",
    "    println(\"Depth: $d -> Acc: $(round(acc, digits=4))\")\n",
    "    \n",
    "    if acc > best_dt_acc\n",
    "        global best_dt_acc = acc\n",
    "        global best_dt_params = params\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\">> DT WINNER: Depth $(best_dt_params[\"max_depth\"]) with Acc: $best_dt_acc\")\n",
    "best_models[:DecisionTree] = best_dt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff731e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Training kNN ---\n",
      "k: 1 -> Acc: 0.9582\n",
      "k: 3 -> Acc: 0.9603\n",
      "k: 5 -> Acc: 0.9602\n",
      "k: 7 -> Acc: 0.9624\n",
      "k: 9 -> Acc: 0.9601\n",
      "k: 11 -> Acc: 0.9668\n",
      ">> kNN WINNER: k=11 with Acc: 0.9668006148440931\n"
     ]
    }
   ],
   "source": [
    "# This cell trains k-Nearest Neighbors (kNN) classifiers by testing 6 different values for 'k'\n",
    "# (number of neighbors). It evaluates each configuration using cross-validation, tracks the \n",
    "# best performing one based on accuracy, and stores the winning parameter in the best_models \n",
    "# dictionary. It explicitly adjusts the dictionary key format to ensure compatibility with \n",
    "# the Ensemble module used later.\n",
    "\n",
    "println(\"\\n--- 4. Training kNN ---\")\n",
    "\n",
    "# 6 k values\n",
    "k_values = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "best_knn_acc = 0.0\n",
    "best_knn_params = nothing\n",
    "\n",
    "for k in k_values\n",
    "    params = Dict(\"n_neighbors\" => k)\n",
    "    metrics, _ = Utils.modelCompilation(:KNeighborsClassifier, params, (x_train, targets_train_vec), cv_indices)\n",
    "    acc = metrics[1]\n",
    "    \n",
    "    println(\"k: $k -> Acc: $(round(acc, digits=4))\")\n",
    "    \n",
    "    if acc > best_knn_acc\n",
    "        global best_knn_acc = acc\n",
    "        global best_knn_params = params\n",
    "    end\n",
    "end\n",
    "\n",
    "# Key adjustment for compatibility with the Ensemble module (requires :K, not :n_neighbors)\n",
    "best_models[:kNN] = Dict(\"K\" => best_knn_params[\"n_neighbors\"])\n",
    "\n",
    "println(\">> kNN WINNER: k=$(best_models[:kNN][\"K\"]) with Acc: $best_knn_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e088a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Training Ensemble (4 Models: SVM, DT, kNN, ANN) ---\n",
      "Training ensemble with the 4 models... (This might take a bit longer due to the ANN)\n",
      ">> ENSEMBLE RESULT (CV 4 Models): Mean Acc = 0.9672\n"
     ]
    }
   ],
   "source": [
    "# This cell trains an Ensemble model combining the four previously optimized algorithms: SVM, \n",
    "# Decision Tree, kNN, and ANN. It defines a helper function to convert parameter keys from \n",
    "# strings to symbols to match the requirements of the training function. The ensemble is \n",
    "# configured to use 'hard voting' (majority rule) and is evaluated using the cross-validation \n",
    "# indices generated earlier. Finally, it reports the average accuracy of the ensemble.\n",
    "\n",
    "println(\"\\n--- 5. Training Ensemble (4 Models: SVM, DT, kNN, ANN) ---\")\n",
    "\n",
    "# Helper function to convert String keys to Symbol\n",
    "# (Necessary because Utils.trainClassEnsemble looks for :topology, not \"topology\")\n",
    "function string_to_symbol_keys(d::Dict)\n",
    "    return Dict(Symbol(k) => v for (k, v) in d)\n",
    "end\n",
    "\n",
    "# 1. Define the 4 estimators\n",
    "estimators_list = [:SVM, :DecisionTree, :kNN, :ANN]\n",
    "\n",
    "# 2. Prepare parameters (converting keys to Symbols)\n",
    "params_list = [\n",
    "    string_to_symbol_keys(best_models[:SVM]),\n",
    "    string_to_symbol_keys(best_models[:DecisionTree]),\n",
    "    string_to_symbol_keys(best_models[:kNN]),\n",
    "    string_to_symbol_keys(best_models[:ANN])\n",
    "]\n",
    "\n",
    "println(\"Training ensemble with the 4 models... (This might take a bit longer due to the ANN)\")\n",
    "\n",
    "# 3. Train the ensemble\n",
    "ensemble_res = Utils.trainClassEnsemble(\n",
    "    estimators_list, \n",
    "    params_list, \n",
    "    Dict(:voting => :hard), # Majority voting\n",
    "    (x_train, y_train),     # Use original targets (one-hot/matrix)\n",
    "    cv_indices\n",
    ")\n",
    "\n",
    "ensemble_acc = ensemble_res.mean_metric\n",
    "println(\">> ENSEMBLE RESULT (CV 4 Models): Mean Acc = $(round(ensemble_acc, digits=4))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab278508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " FINAL APPROACH EVALUATION (TEST SET)\n",
      "==================================================\n",
      "The winning model in Cross-Validation was: ANN (Acc: 0.9782)\n",
      "\n",
      "--- Normalizing data for final training ---\n",
      "Data normalized. Mean feature 1 (Train): 0.33629612746429\n",
      "\n",
      "--- Training the Winner with ALL Train Set and Evaluating on Test Set ---\n",
      "Retraining ANN with Utils...\n",
      "(Creating an internal 20% validation split from x_train_final for Early Stopping)\n",
      "Confusion Matrix:\n",
      "[70 0; 2 41]\n",
      "\n",
      "Accuracy: 0.9823008849557522\n",
      "Error rate: 0.017699115044247787\n",
      "Sensitivity: 0.9534883720930233\n",
      "Specificity: 1.0\n",
      "PPV: 1.0\n",
      "NPV: 0.9722222222222222\n",
      "F1-score: 0.9761904761904763\n"
     ]
    }
   ],
   "source": [
    "# This cell performs the final evaluation of the best model found. It compares the \n",
    "# cross-validation accuracies to select the \"winner\". \n",
    "# CRUCIAL STEP: Before training the winner, it calculates normalization parameters \n",
    "# using the FULL training set (x_train) and applies them to create normalized \n",
    "# versions of the training and test sets. Then, it retrains the winner.\n",
    "\n",
    "println(\"\\n\" * \"=\"^50)\n",
    "println(\" FINAL APPROACH EVALUATION (TEST SET)\")\n",
    "println(\"=\"^50)\n",
    "\n",
    "# --- NECESSARY IMPORTS ---\n",
    "import LIBSVM\n",
    "import DecisionTree\n",
    "import NearestNeighborModels\n",
    "import MLJFlux\n",
    "using Flux \n",
    "using CategoricalArrays \n",
    "using Random\n",
    "using MLJ\n",
    "\n",
    "# 1. Compare cross-validation results to choose the champion\n",
    "accuracies = [best_ann_acc, best_svm_acc, best_dt_acc, best_knn_acc, ensemble_acc]\n",
    "model_names = [\"ANN\", \"SVM\", \"DecisionTree\", \"kNN\", \"Ensemble\"]\n",
    "\n",
    "best_idx = argmax(accuracies)\n",
    "winner_name = model_names[best_idx]\n",
    "println(\"The winning model in Cross-Validation was: $winner_name (Acc: $(round(accuracies[best_idx], digits=4)))\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. DATA NORMALIZATION FOR FINAL TRAINING\n",
    "# ---------------------------------------------------------\n",
    "println(\"\\n--- Normalizing data for final training ---\")\n",
    "\n",
    "# Create copies to hold the normalized data\n",
    "x_train_final = copy(x_train) # Starts as raw\n",
    "x_test_final  = copy(x_test)  # Starts as raw\n",
    "\n",
    "# Calculate parameters based ONLY on Training Data\n",
    "normParams = Utils.calculateMinMaxNormalizationParameters(x_train_final)\n",
    "\n",
    "# Apply normalization\n",
    "Utils.normalizeMinMax!(x_train_final, normParams)\n",
    "Utils.normalizeMinMax!(x_test_final, normParams)\n",
    "\n",
    "println(\"Data normalized. Mean feature 1 (Train): $(mean(x_train_final[:,1]))\")\n",
    "\n",
    "println(\"\\n--- Training the Winner with ALL Train Set and Evaluating on Test Set ---\")\n",
    "\n",
    "# Set seed for reproducibility of the final training\n",
    "Random.seed!(42)\n",
    "\n",
    "y_pred_final = nothing\n",
    "\n",
    "if winner_name == \"SVM\"\n",
    "    p = best_models[:SVM]\n",
    "    kernel_map = Dict(\n",
    "        \"linear\" => LIBSVM.Kernel.Linear, \"rbf\" => LIBSVM.Kernel.RadialBasis,\n",
    "        \"poly\" => LIBSVM.Kernel.Polynomial, \"sigmoid\" => LIBSVM.Kernel.Sigmoid\n",
    "    )\n",
    "    chosen_kernel = kernel_map[p[\"kernel\"]]\n",
    "    model_type = MLJ.@load ProbabilisticSVC pkg=LIBSVM verbosity=0\n",
    "    model = model_type(kernel=chosen_kernel, cost=Float64(p[\"C\"]))\n",
    "    # Use x_train_final\n",
    "    mach = machine(model, MLJ.table(x_train_final), categorical(targets_train_vec))\n",
    "    fit!(mach, verbosity=0)\n",
    "    y_pred_final = mode.(predict(mach, MLJ.table(x_test_final)))\n",
    "\n",
    "elseif winner_name == \"DecisionTree\"\n",
    "    p = best_models[:DecisionTree]\n",
    "    model_type = MLJ.@load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "    model = model_type(max_depth=p[\"max_depth\"], rng=Random.MersenneTwister(42))\n",
    "    # Use x_train_final\n",
    "    mach = machine(model, MLJ.table(x_train_final), categorical(targets_train_vec))\n",
    "    fit!(mach, verbosity=0)\n",
    "    y_pred_final = mode.(predict(mach, MLJ.table(x_test_final)))\n",
    "\n",
    "elseif winner_name == \"kNN\"\n",
    "    p = best_models[:kNN]\n",
    "    model_type = MLJ.@load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "    k_val = haskey(p, \"K\") ? p[\"K\"] : p[\"n_neighbors\"]\n",
    "    model = model_type(K=k_val)\n",
    "    # Use x_train_final\n",
    "    mach = machine(model, MLJ.table(x_train_final), categorical(targets_train_vec))\n",
    "    fit!(mach, verbosity=0)\n",
    "    y_pred_final = mode.(predict(mach, MLJ.table(x_test_final)))\n",
    "\n",
    "elseif winner_name == \"ANN\"\n",
    "    println(\"Retraining ANN with Utils...\")\n",
    "    println(\"(Creating an internal 20% validation split from x_train_final for Early Stopping)\")\n",
    "    p = best_models[:ANN]\n",
    "    \n",
    "    # --- INTERNAL SPLIT FOR ANN ---\n",
    "    # We split x_train_final into 80% train / 20% validation just for this training\n",
    "    n_train_final = size(x_train_final, 1)\n",
    "    t_idx, v_idx = Utils.holdOut(n_train_final, 0.2) # 20% for validation\n",
    "    \n",
    "    x_t_ann = x_train_final[t_idx, :]\n",
    "    y_t_ann = y_train[t_idx, :]\n",
    "    \n",
    "    x_v_ann = x_train_final[v_idx, :]\n",
    "    y_v_ann = y_train[v_idx, :]\n",
    "    \n",
    "    # Train using the sub-split\n",
    "    ann, _, _, _ = Utils.trainClassANN(\n",
    "        p[\"topology\"],\n",
    "        (x_t_ann, y_t_ann), \n",
    "        validationDataset=(x_v_ann, y_v_ann), # Internal validation set passed here\n",
    "        maxEpochs=p[\"maxEpochs\"],\n",
    "        learningRate=p[\"learningRate\"]\n",
    "    )\n",
    "    \n",
    "    # Manual prediction on x_test_final\n",
    "    y_pred_raw = ann(x_test_final')'\n",
    "    y_pred_bool_manual = vec(Utils.classifyOutputs(y_pred_raw))\n",
    "    Utils.printConfusionMatrix(y_pred_bool_manual, targets_test_vec)\n",
    "    y_pred_final = nothing \n",
    "\n",
    "elseif winner_name == \"Ensemble\"\n",
    "    println(\"Rebuilding Ensemble...\")\n",
    "    # 1. Base SVM\n",
    "    p_svm = best_models[:SVM]\n",
    "    svm_k = Dict(\"linear\"=>LIBSVM.Kernel.Linear, \"rbf\"=>LIBSVM.Kernel.RadialBasis, \"poly\"=>LIBSVM.Kernel.Polynomial, \"sigmoid\"=>LIBSVM.Kernel.Sigmoid)[p_svm[\"kernel\"]]\n",
    "    SVC = MLJ.@load ProbabilisticSVC pkg=LIBSVM verbosity=0\n",
    "    m1 = SVC(kernel=svm_k, cost=Float64(p_svm[\"C\"]))\n",
    "    # 2. Base DT\n",
    "    p_dt = best_models[:DecisionTree]\n",
    "    DTC = MLJ.@load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "    m2 = DTC(max_depth=p_dt[\"max_depth\"])\n",
    "    # 3. Base kNN\n",
    "    p_knn = best_models[:kNN]\n",
    "    KNN = MLJ.@load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "    m3 = KNN(K=p_knn[\"K\"])\n",
    "    # 4. Base ANN\n",
    "    p_ann = best_models[:ANN]\n",
    "    NNC = MLJ.@load NeuralNetworkClassifier pkg=MLJFlux verbosity=0\n",
    "    builder = MLJFlux.Short(n_hidden=p_ann[\"topology\"][1], σ=Flux.relu) \n",
    "    m4 = NNC(builder=builder, epochs=200) \n",
    "    \n",
    "    ensemble_model = Utils.VotingClassifier(models=[m1, m2, m3, m4], voting=:hard)\n",
    "    # Use x_train_final\n",
    "    mach = machine(ensemble_model, MLJ.table(x_train_final), categorical(targets_train_vec))\n",
    "    fit!(mach, verbosity=0)\n",
    "    y_pred_final = predict_mode(mach, MLJ.table(x_test_final))\n",
    "end\n",
    "\n",
    "if !isnothing(y_pred_final)\n",
    "    println(\"\\n--- FINAL CONFUSION MATRIX (TEST SET) ---\")\n",
    "    y_pred_bool = [x == \"true\" || x == true for x in MLJ.unwrap.(y_pred_final)]\n",
    "    Utils.printConfusionMatrix(y_pred_bool, targets_test_vec)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
